{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fleet-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "from matplotlib.ticker import NullFormatter  # useful for `logit` scale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import PySimpleGUI as sg\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "import os.path\n",
    "\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scrapy import cmdline\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "\n",
    "#--------------------------------pdf downloader import ----------------------------\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager #updated\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "\n",
    "from IPython.display import display_html\n",
    "\n",
    "def env_import():\n",
    "    from matplotlib.ticker import NullFormatter  # useful for `logit` scale\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "    import PySimpleGUI as sg\n",
    "    import matplotlib\n",
    "    matplotlib.use('TkAgg')\n",
    "\n",
    "    import PySimpleGUI as sg\n",
    "    import os.path\n",
    "\n",
    "\n",
    "    import PyPDF2\n",
    "\n",
    "\n",
    "    import nltk\n",
    "    from nltk.corpus import PlaintextCorpusReader\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "    from collections import OrderedDict\n",
    "    import pprint\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import os\n",
    "\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from scrapy import cmdline\n",
    "    import json\n",
    "    from pandas.io.json import json_normalize\n",
    "\n",
    "\n",
    "#--------------------------------pdf downloader import ----------------------------\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager #updated\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "\n",
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------End of import ------------------------------------\n",
    "\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "#-------------PDF downloader function -----------------------\n",
    "class pdfDownloader:\n",
    "    def __init__(self, json_file = 'items.json'):\n",
    "        self.file_path = json_file\n",
    "        pass\n",
    "\n",
    "    def download(self):\n",
    "        pdf_count = 0\n",
    "        data_str = open(self.file_path).read()\n",
    "        data_str='['+data_str[:-2]+']'\n",
    "        data_list = json.loads(data_str)\n",
    "\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        browser = webdriver.Chrome(ChromeDriverManager().install(), chrome_options = chrome_options)\n",
    "        for item in data_list:\n",
    "            company_name = item['company']\n",
    "            comp_initial = item['url'].split('/')[2]\n",
    "            index = comp_initial.find('.')\n",
    "            comp_initial = comp_initial[index+1:]\n",
    "\n",
    "            current_directory = os.getcwd()\n",
    "            final_directory = os.path.join(current_directory, 'reports/'+company_name)\n",
    "            if not os.path.exists(final_directory):\n",
    "               os.makedirs(final_directory)\n",
    "\n",
    "            browser.get(item['url'])\n",
    "            links = get_links_onpage(browser)\n",
    "            for lin in links:\n",
    "                if ('pdf' in lin) and (comp_initial in lin):\n",
    "                    pdf_count += 1\n",
    "                    cur_file = os.listdir(final_directory)\n",
    "                    if lin.split('/')[-1][:-4] + '.pdf' not in cur_file:\n",
    "                        try:\n",
    "                            download_file(lin, final_directory, lin.split('/')[-1][:-4])\n",
    "                        except:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "        browser.quit()\n",
    "\n",
    "        #searching for additional reports\n",
    "        for item in data_list:\n",
    "            company_name = item['company']\n",
    "            break\n",
    "\n",
    "\n",
    "        comp_initial = item['url'].split('/')[2]\n",
    "        search_string = comp_initial.split('.')[1]\n",
    "        comp_name = search_string\n",
    "\n",
    "        #print(comp_name)\n",
    "        # This is done to structure the string\n",
    "        # into search url.(This can be ignored)\n",
    "        search_string = search_string + \" sustainability report\"\n",
    "        search_string = search_string.replace(' ', '+')\n",
    "\n",
    "        # Assigning the browser variable with chromedriver of Chrome.\n",
    "        # Any other browser and its respective webdriver\n",
    "        # like geckodriver for Mozilla Firefox can be used\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        browser = webdriver.Chrome(ChromeDriverManager().install(), options = chrome_options)\n",
    "\n",
    "        for i in range(1):\n",
    "            matched_elements = browser.get(\"https://www.google.com/search?q=\" + search_string + \"&start=\" + str(i))\n",
    "\n",
    "        links_fst = get_links_onpage(browser)\n",
    "\n",
    "        google_website_visited = 0\n",
    "        ##entering the google website\n",
    "        while google_website_visited < 1:\n",
    "            for lin in links_fst:\n",
    "                pdf_exist = 0\n",
    "                google_website_visited += 1\n",
    "                download_count = 0\n",
    "                if \"google\" in lin:\n",
    "                    continue\n",
    "                elif (comp_name + \".com\" in lin and 'sustainability' in lin.split('/')[-2]) or\\\n",
    "                (comp_name + \".com\" in lin and 'responsibilities' in lin.split('/')[-2]) or\\\n",
    "                ('our' in lin and 'sustainability' in lin.split('/')[-2]) or\\\n",
    "                (comp_name + \".com\" in lin and 'sustainable' in lin.split('/')[-2]) or \\\n",
    "                (comp_name + \".com\" in lin and 'sustainability' in lin.split('/')[-1]) or\\\n",
    "                (comp_name + \".com\" in lin and 'csr' in lin.split('/')[-2]): #last one for at&t\n",
    "                    browser.get(lin)\n",
    "                    time.sleep(2)\n",
    "                    ##entering the companies website\n",
    "                    links_snd = get_links_onpage(browser)\n",
    "                    for lin in links_snd:\n",
    "                        if 'pdf' in lin:\n",
    "                            #print(lin)\n",
    "                            pdf_exist = 1\n",
    "                            cur_file = os.listdir(final_directory)\n",
    "                            if lin.split('/')[-1][:-4] + '.pdf' not in cur_file:\n",
    "                                try:\n",
    "                                    download_file(lin, final_directory, lin.split('/')[-1][:-4])\n",
    "                                    download_count += 1\n",
    "                                except:\n",
    "                                    continue\n",
    "                            else:\n",
    "                                continue\n",
    "                    if download_count > 0 or pdf_exist == 1:\n",
    "                        browser.quit()\n",
    "                        break\n",
    "                    else:\n",
    "                        for lin in links_snd:\n",
    "                            if 'report' in lin:\n",
    "                                #print(lin)\n",
    "                                browser.get(lin)\n",
    "                                links_trd = get_links_onpage(browser)\n",
    "                                for lin in links_trd:\n",
    "                                    if 'pdf' in lin:\n",
    "                                        cur_file = os.listdir(final_directory)\n",
    "                                        if lin.split('/')[-1][:-4] + '.pdf' not in cur_file:\n",
    "                                            try:\n",
    "                                                download_file(lin, final_directory, lin.split('/')[-1][:-4])\n",
    "                                                download_count += 1\n",
    "                                            except:\n",
    "                                                continue\n",
    "                                        else:\n",
    "                                            continue\n",
    "                                if download_count > 0 or pdf_exist == 1:\n",
    "                                    browser.quit()\n",
    "                                    break\n",
    "                    break\n",
    "            break\n",
    "\n",
    "        if download_count == 0:\n",
    "            browser.quit()\n",
    "            #print('no new report found for the company :( !')\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "##utility functions\n",
    "\n",
    "class AppURLopener(urllib.request.FancyURLopener):\n",
    "    version = \"Mozilla/5.0\"\n",
    "\n",
    "#opener = AppURLopener()\n",
    "#response = opener.open('http://httpbin.org/user-agent')\n",
    "\n",
    "\n",
    "def download_file(download_url, final_directory, filename):\n",
    "    try:\n",
    "        response = urllib.request.urlopen(download_url)\n",
    "    except:\n",
    "\n",
    "        opener = AppURLopener()\n",
    "        response = opener.open(download_url)\n",
    "    file = open(final_directory + '/' + filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    "\n",
    "def get_links_onpage(driver):\n",
    "    links = []\n",
    "    elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "    for elem in elems:\n",
    "        lin = elem.get_attribute(\"href\")\n",
    "        links.append(lin)\n",
    "    return links\n",
    "\n",
    "\n",
    "\n",
    "#-------------WordCloud functions -------------------------\n",
    "DELETE_WORDS = []\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "#Remove short words\n",
    "MIN_LENGTH = 0\n",
    "def remove_short_words(text_string,min_length = MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1)\n",
    "    return text_string\n",
    "\n",
    "def import_keyword(file):\n",
    "    import csv\n",
    "    with open(file,newline = '') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        data.pop(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def wordcloud_drawer(fname, perfect_text ='no text'):\n",
    "\n",
    "    if perfect_text == 'no text':\n",
    "        f=open(fname,\"r\")\n",
    "        temp = f.read().splitlines()\n",
    "        text = ' '.join(temp)\n",
    "    else:\n",
    "        text = perfect_text\n",
    "\n",
    "    text_string = remove_words(text)\n",
    "    text_string = remove_short_words(text_string)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=500,height=500).generate(text_string)\n",
    "\n",
    "    wordcloud.to_file('wordcloud.png')\n",
    "\n",
    "def keywordcloud_drawer(fname, perfect_text ='no text'):\n",
    "\n",
    "    if perfect_text == 'no text':\n",
    "        f=open(fname,\"r\")\n",
    "        temp = f.read().splitlines()\n",
    "        text = ' '.join(temp)\n",
    "    else:\n",
    "        text = perfect_text\n",
    "\n",
    "    text_string = remove_words(text)\n",
    "    text_string = remove_short_words(text_string)\n",
    "\n",
    "    keyword = import_keyword('keyword.csv')\n",
    "    words = nltk.Text(word_tokenize(text_string))\n",
    "    result = list()\n",
    "    for word in words:\n",
    "        for kw in keyword:\n",
    "            if str(word) == str(kw[0]):\n",
    "                result.append(str(kw[0]))\n",
    "    clean_up_string = ' '.join(word for word in result)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=500,height=500).generate(clean_up_string)\n",
    "    wordcloud.to_file('keywordcloud.png')\n",
    "\n",
    "#--------------------------------PDF convertor function ------------------\n",
    "def read_in_pdf(input_name):\n",
    "    pdffileobj=open(input_name,'rb')\n",
    "    pdfreader=PyPDF2.PdfFileReader(pdffileobj, strict=False)\n",
    "    x=pdfreader.numPages\n",
    "    try:\n",
    "        os.remove(\"intermediate_file1.txt\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    for i in range(x):\n",
    "        pageobj=pdfreader.getPage(i)\n",
    "        text=pageobj.extractText()\n",
    "\n",
    "        file1=open(r\"intermediate_file1.txt\",\"a\", encoding=\"utf-8\")\n",
    "        file1.writelines(text)\n",
    "        file1.close()\n",
    "    f=open(r\"intermediate_file1.txt\",\"r\", encoding=\"utf-8\")\n",
    "    temp = f.read().splitlines()\n",
    "    text = ' '.join(temp)\n",
    "    return text\n",
    "\n",
    "#---------------------------Sentence Embedding --------------------------\n",
    "\n",
    "\n",
    "\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "# text=read_in_pdf('coty_sustainability_report_fy20.pdf')\n",
    "\n",
    "\n",
    "def sentence_embedding(input_query,fname, perfect_text ='no text'):\n",
    "\n",
    "\n",
    "    if perfect_text == 'no text':\n",
    "        f=open(fname,\"r\")\n",
    "        temp = f.read().splitlines()\n",
    "        text = ' '.join(temp)\n",
    "    else:\n",
    "        text = perfect_text\n",
    "\n",
    "\n",
    "    sentences = nltk.Text(sent_tokenize(text))\n",
    "    words = nltk.Text(word_tokenize(text))\n",
    "\n",
    "    tokenized_sent = []\n",
    "    for s in sentences:\n",
    "        tokenized_sent.append(word_tokenize(s.lower()))\n",
    "\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "    model = hub.load(module_url)\n",
    "    # print (\"module %s loaded\" % module_url)\n",
    "\n",
    "    sentence_embeddings = model(sentences)\n",
    "    query = input_query\n",
    "    query_vec = model([query])[0]\n",
    "\n",
    "    df= pd.DataFrame()\n",
    "\n",
    "    sentence_list = list()\n",
    "    sim_list= list()\n",
    "\n",
    "    for sent in sentences:\n",
    "        sim = cosine(query_vec, model([sent])[0])\n",
    "        sentence_list.append(sent)\n",
    "        sim_list.append(sim)\n",
    "\n",
    "    df['sentence'] = sentence_list\n",
    "    df['similarity'] = sim_list\n",
    "    pd.set_option('display.max_colwidth', 500)\n",
    "    df =df.sort_values(by=['similarity'],ascending = False)\n",
    "\n",
    "    return(df['sentence'].head(5))\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------- Beginning of GUI CODE -------------------------------\n",
    "\n",
    "# define the window layout\n",
    "sg.change_look_and_feel('GreenMono')\n",
    "file_list_column = [\n",
    "    [\n",
    "        sg.Text(\"Sustainability Folder\"),\n",
    "        sg.In(size=(25, 1), enable_events=True, key=\"-FOLDER-\"),\n",
    "        sg.FolderBrowse(),\n",
    "    ],\n",
    "    [\n",
    "        sg.Listbox(\n",
    "            values=[], enable_events=True, size=(40, 10), key=\"-FILE LIST-\"\n",
    "        )\n",
    "    ],\n",
    "    [   sg.Text(\"Input query\"),\n",
    "        sg.Input(key='_IN_')],\n",
    "    [\n",
    "        sg.Output(size=(60,15))\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        sg.Button(\"Run\")\n",
    "    ]\n",
    "]\n",
    "\n",
    "plot_column = [\n",
    "          [sg.Text(\"Input Company\"),\n",
    "           sg.Input(key='_IN3_'),\n",
    "           sg.Button('DownloadPDF'),\n",
    "           sg.Button('Downloader')],\n",
    "          [sg.Text('WordCloud')],\n",
    "          [sg.Image(key='-IMAGE-')],\n",
    "          [sg.Button('WordCloud'),sg.Button('Keyword Cloud')]\n",
    "          ]\n",
    "\n",
    "\n",
    "layout = [\n",
    "    [\n",
    "        sg.Column(file_list_column),\n",
    "        sg.VSeperator(),\n",
    "        sg.Column(plot_column),\n",
    "    ]\n",
    "]\n",
    "# create the form and show it without the plot\n",
    "window = sg.Window('Sustainability Report Interface Beta version', layout,size=(1270, 550), grab_anywhere=True, resizable=True,finalize=True, element_justification='center', font='Helvetica 14')\n",
    "\n",
    "# add the plot to the window\n",
    "# fig_canvas_agg = draw_figure(window['-CANVAS-'].TKCanvas, fig)\n",
    "\n",
    "\n",
    "# ----------------------sustainability functions\n",
    "# def read_in_pdf(input_name):\n",
    "#     pdffileobj=open(input_name,'rb')\n",
    "#     pdfreader=PyPDF2.PdfFileReader(pdffileobj, strict=False)\n",
    "#     x=pdfreader.numPages\n",
    "#     try:\n",
    "#         os.remove(\"intermediate_file1.txt\")\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     for i in range(x):\n",
    "#         pageobj=pdfreader.getPage(i)\n",
    "#         text=pageobj.extractText()\n",
    "#\n",
    "#         file1=open(r\"intermediate_file1.txt\",\"a\")\n",
    "#         file1.writelines(text)\n",
    "#         file1.close()\n",
    "#     f=open(r\"intermediate_file1.txt\",\"r\")\n",
    "#     temp = f.read().splitlines()\n",
    "#     text = ' '.join(temp)\n",
    "#     return text\n",
    "\n",
    "\n",
    "print(\"Welcome to our Sustainability Radar!\")\n",
    "print(\"New features will be added soon!\")\n",
    "print(\"Enjoy!\")\n",
    "print('--------------------------------------------------------------------------------')\n",
    "\n",
    "# -------Conditional Statement Starts Here --------------------------------------------\n",
    "count = 0\n",
    "while True:\n",
    "\n",
    "    event, values = window.read()\n",
    "    if event == \"Exit\" or event == sg.WIN_CLOSED:\n",
    "        break\n",
    "\n",
    "# -------Directory Section ----------\n",
    "    if event == \"-FOLDER-\":\n",
    "        folder = values[\"-FOLDER-\"]\n",
    "        try:\n",
    "            # Get list of files in folder\n",
    "            file_list = os.listdir(folder)\n",
    "        except:\n",
    "            file_list = []\n",
    "\n",
    "        fnames = [\n",
    "            f\n",
    "            for f in file_list\n",
    "            if os.path.isfile(os.path.join(folder, f))\n",
    "            and f.lower().endswith((\".pdf\", \".txt\"))\n",
    "        ]\n",
    "        window[\"-FILE LIST-\"].update(fnames)\n",
    "    elif event == \"-FILE LIST-\":  # A file was chosen from the listbox\n",
    "        try:\n",
    "            filename = os.path.join(\n",
    "                values[\"-FOLDER-\"], values[\"-FILE LIST-\"][0]\n",
    "            )\n",
    "            #window[\"-TOUT-\"].update(filename)\n",
    "            print(\"Current selected file: \"+filename)\n",
    "            selected_file = filename\n",
    "        except:\n",
    "            pass\n",
    "#---------- sentence embedding ----------\n",
    "    if event == \"Run\":\n",
    "        # text=read_in_pdf('coty_sustainability_report_fy20.pdf')\n",
    "        # sentences = nltk.Text(sent_tokenize(text))\n",
    "        # print(len(sentences))\n",
    "        try:\n",
    "            print(\"####################################################\")\n",
    "            if 'pdf' in str(selected_file):\n",
    "                from_pdf_to_txt = read_in_pdf(str(selected_file))\n",
    "                SE = sentence_embedding(input_query = values['_IN_'],fname = str(selected_file),perfect_text = from_pdf_to_txt)\n",
    "            else:\n",
    "                SE = sentence_embedding(input_query = values['_IN_'],fname = str(selected_file))\n",
    "            sent_count = 0\n",
    "            for i in SE:\n",
    "                print('Sentence '+str(sent_count)+': '+i)\n",
    "                sent_count += 1\n",
    "#         print(sentence_embedding(input_query = values['_IN_']))\n",
    "            print(\"####################################################\")\n",
    "        except:\n",
    "            print(\"Please choose the file to process.\")\n",
    "        # print(sentence_embedding())\n",
    "        # if window:\n",
    "        #     window.Refresh()\n",
    "\n",
    "# -------WordCloud Section-------------\n",
    "    if event == \"WordCloud\":\n",
    "        try:\n",
    "            if 'pdf' in str(selected_file):\n",
    "                from_pdf_to_txt = read_in_pdf(str(selected_file))\n",
    "                wordcloud_drawer(str(selected_file),perfect_text = from_pdf_to_txt)\n",
    "            else:\n",
    "                wordcloud_drawer(str(selected_file))\n",
    "            window[\"-IMAGE-\"].update(filename='wordcloud.png')\n",
    "        except:\n",
    "            print(\"Please choose the file to process.\")\n",
    "    if event == \"Keyword Cloud\":\n",
    "        try:\n",
    "            if 'pdf' in str(selected_file):\n",
    "                from_pdf_to_txt = read_in_pdf(str(selected_file))\n",
    "                keywordcloud_drawer(str(selected_file),perfect_text = from_pdf_to_txt)\n",
    "            else:\n",
    "                keywordcloud_drawer(str(selected_file))\n",
    "            window[\"-IMAGE-\"].update(filename='keywordcloud.png')\n",
    "        except:\n",
    "            print(\"Please choose the file to process.\")\n",
    "\n",
    "\n",
    "    if event == \"DownloadPDF\":\n",
    "        try:\n",
    "            print('###########################################')\n",
    "            print('Downloading')\n",
    "            print('This process may take more than 10 minutes.')\n",
    "#             try:\n",
    "#                 cmdline.execute('scrapy crawl Sustn'.split())\n",
    "#             except:\n",
    "#                 print(\"Json already downloaded\")\n",
    "\n",
    "\n",
    "            p = pdfDownloader()\n",
    "            p.download()\n",
    "            print('Downloading Finished')\n",
    "        except:\n",
    "            print('Download Error!')\n",
    "\n",
    "    if event == \"Downloader\":\n",
    "        os.system('python3 PDF_scrappy.py')\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         os.rmdir('keywordcloud.png')\n",
    "#         os.rmdir('wordcloud.png')\n",
    "#     except OSError as e:\n",
    "#         print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "window.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-government",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
